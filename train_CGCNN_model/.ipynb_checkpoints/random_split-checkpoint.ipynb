{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document demonstrates the making, training, saving, loading, and usage of a sklearn-compliant CGCNN model.\n",
    "\n",
    "The dataset is randomly spilt into 8:2 training: test set in this script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'cgcnn/') # you will need to clone the CGCNN repository and add the path here\n",
    "import mongo\n",
    "import cgcnn\n",
    "\n",
    "from cgcnn.data import collate_pool, MergeDataset, StructureDataTransformer\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../surface_energy_dataset/surface_energy_dataset.pkl','rb'))\n",
    "docs = data['docs']\n",
    "sort = data['sort']\n",
    "random.seed(123)\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Process  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT = StructureDataTransformer(atom_init_loc='atom_init.json',\n",
    "                               max_num_nbr=12,\n",
    "                               step=0.2,\n",
    "                               radius=8,\n",
    "                               use_voronoi=False,\n",
    "                               use_tag=False,\n",
    "                               use_fixed_info=False,\n",
    "                               use_distance=False,\n",
    "                               train_geometry = 'initial'\n",
    "                               )\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the target list\n",
    "# We kept the index of each documents in the list so we can keep track of them after they are shuffled\n",
    "target_list = np.array([[int(docs.index(doc)),np.log(doc['intercept'])] for doc in docs])\n",
    "target_list = pd.DataFrame(target_list, columns = ['doc_index', 'intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and split the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_training, SDT_test, target_training, target_test = train_test_split(SDT_list, target_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CGCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further spilt the training data into train and validate set by 8:2 ratio for CGCNN\n",
    "train_test_splitter = ShuffleSplit(test_size=0.2, random_state=42)\n",
    "LR_schedule = LRScheduler('MultiStepLR',milestones=[100],gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=87,  \n",
    "    module__classification=False,\n",
    "    lr=np.exp(-6.465085550816676),     \n",
    "    max_epochs=218,\n",
    "    module__atom_fea_len=43,\n",
    "    module__h_fea_len=114,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=3, \n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_train__shuffle=True, #VERY IMPORTANT\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    iterator_valid__shuffle=False, #This should be False, which is the default\n",
    "    device=device,\n",
    "   criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.fit(SDT_training,np.array(target_training[['intercept']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further spilt training set into train & validation set to look at the prediction accuracy \n",
    "SDT_train, SDT_valid, target_train, target_valid = train_test_split(SDT_training, target_training, test_size=0.2, random_state=42)\n",
    "\n",
    "training_data = {'doc_index': list(target_train['doc_index']),\n",
    "                 'actual_value':np.exp(target_train['intercept']),\n",
    "                 'predicted_value':np.exp(net.predict(SDT_train).reshape(-1))}\n",
    "\n",
    "validation_data = {'doc_index': list(target_valid['doc_index']),\n",
    "                   'actual_value':np.exp(target_valid['intercept']),\n",
    "                   'predicted_value':np.exp(net.predict(SDT_valid).reshape(-1))}\n",
    "\n",
    "test_data = {'doc_index': list(target_test['doc_index']),\n",
    "            'actual_value':np.exp(target_test['intercept']),\n",
    "            'predicted_value':np.exp(net.predict(SDT_test).reshape(-1))}\n",
    "\n",
    "df_training = pd.concat([pd.DataFrame(train_data), pd.DataFrame(validation_data)])\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# df_training.to_csv('../analyze_prediction_results/CGCNN_prediction_results/randomsplit_training.csv', sep='\\t')\n",
    "# df_test.to_csv('/analyze_prediction_results/CGCNN_prediction_results/randomsplit_test.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
