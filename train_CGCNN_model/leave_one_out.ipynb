{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document demonstrates the making, training, saving, loading, and usage of a sklearn-compliant CGCNN model.\n",
    "\n",
    "The test set is a composition of interest (e.g. CuAl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your composition of interest in the list below, \n",
    "# and keep each element as a seperate string. \n",
    "# Ex: ['Cu','Al'], ['Ni','Ga']\n",
    "composition_of_interest = ['Cu','Al']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'cgcnn/') # you will need to clone the CGCNN repository and add the path here\n",
    "import mongo\n",
    "import cgcnn\n",
    "\n",
    "from cgcnn.data import collate_pool, MergeDataset, StructureDataTransformer\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../surface_energy_dataset/surface_energy_dataset.pkl','rb'))\n",
    "docs = data['docs']\n",
    "sort = data['sort']\n",
    "random.seed(123)\n",
    "random.shuffle(docs)\n",
    "\n",
    "# Add index of each doc to doc to keep track of them (since the data will be shuffled)\n",
    "for index, doc in enumerate(docs):\n",
    "    doc['index'] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selectively Decide Test Data\n",
    "Using leave one out method to see performance on unseen surface patterns. e.g. CuAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveSplit(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    Assign data selectively to training, and test dataset. \n",
    "    Test set is purely composition of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, docs, seed=42):\n",
    "        self.docs = docs\n",
    "        self.seed = seed\n",
    "        self.docs_train = None\n",
    "        self.docs_test = None\n",
    "\n",
    "    def split_docs_by_elememts(self, docs, elements):\n",
    "        \"\"\"\n",
    "        This select all surfaces with desired elements compositions \n",
    "        into test dataset.\n",
    "        Ex:['Cu', 'Al']\n",
    "        \"\"\"\n",
    "        docs_test = [doc for doc in docs \n",
    "                     if (set(doc['initial_configuration']['atoms']['chemical_symbols']) == set(elements))]\n",
    "        docs_train = [doc for doc in docs if doc not in docs_test]\n",
    "        random.seed(self.seed)\n",
    "        random.shuffle(docs_train)\n",
    "        \n",
    "        self.docs_test = docs_test\n",
    "        self.docs_train = docs_train\n",
    "        return self.docs_test, self.docs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SelectiveSplit(docs)\n",
    "docs_test, docs_train = ss.split_docs_by_elememts(docs, composition_of_interest)\n",
    "print('%d test data'%(len(ss.docs_test)))\n",
    "print('%d train data'%(len(ss.docs_train)))\n",
    "\n",
    "training_docs = ss.docs_train\n",
    "test_docs = ss.docs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spilt the prediction labels into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_training = np.array([[doc['index'],np.log(doc['intercept'])] for doc in training_docs])\n",
    "target_training = pd.DataFrame(target_training, columns = ['doc_index', 'intercept'])\n",
    "\n",
    "target_test = np.array([[doc['index'],np.log(doc['intercept'])] for doc in test_docs])\n",
    "target_test = pd.DataFrame(target_test, columns = ['doc_index', 'intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare inputs used in cgcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_from_docs(docs):\n",
    "    SDT = StructureDataTransformer(atom_init_loc='atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step=0.2,\n",
    "                              radius=8,\n",
    "                              use_voronoi=False,\n",
    "                              use_tag=False,\n",
    "                              use_fixed_info=False,\n",
    "                              use_distance=False,\n",
    "                              train_geometry = 'initial'\n",
    "                              )\n",
    "\n",
    "    SDT_out = SDT.transform(docs)\n",
    "    structures = SDT_out[0]\n",
    "\n",
    "    #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "    SDT_out = SDT.transform(docs)\n",
    "    with mp.Pool(4) as pool:\n",
    "        SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "        \n",
    "    return SDT_list, orig_atom_fea_len, nbr_fea_len\n",
    "\n",
    "\n",
    "SDT_training, orig_atom_fea_len, nbr_fea_len = prepare_input_from_docs(training_docs)\n",
    "SDT_test,_,_  = prepare_input_from_docs(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',\n",
    "                fn_prefix='%s_valid_best_'%''.join(composition_of_interest))\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('%s_valid_best_params.pt'%''.join(composition_of_interest))\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CGCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splitter = ShuffleSplit(test_size=0.2, random_state=42)\n",
    "LR_schedule = LRScheduler('MultiStepLR',milestones=[100],gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=87,  \n",
    "    module__classification=False,\n",
    "    lr=np.exp(-6.465085550816676),     \n",
    "    max_epochs=218,\n",
    "    module__atom_fea_len=43,\n",
    "    module__h_fea_len=114,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=3, \n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_train__shuffle=True, #VERY IMPORTANT\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    iterator_valid__shuffle=False, #This should be False, which is the default\n",
    "    device=device,\n",
    "   criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.fit(SDT_training,np.array(target_training[['intercept']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Make predictions and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further spilt training set into train & validation set to look at the prediction accuracy \n",
    "SDT_train, SDT_valid, target_train, target_valid = train_test_split(SDT_training, target_training, test_size=0.25, random_state=42)\n",
    "\n",
    "train_data = {'doc_index': list(target_train['doc_index']),\n",
    "                 'actual_value':np.exp(target_train['intercept']),\n",
    "                 'predicted_value':np.exp(net.predict(SDT_train).reshape(-1))}\n",
    "\n",
    "validation_data = {'doc_index': list(target_valid['doc_index']),\n",
    "                   'actual_value':np.exp(target_valid['intercept']),\n",
    "                   'predicted_value':np.exp(net.predict(SDT_valid).reshape(-1))}\n",
    "\n",
    "test_data = {'doc_index': list(target_test['doc_index']),\n",
    "            'actual_value':np.exp(target_test['intercept']),\n",
    "            'predicted_value':np.exp(net.predict(SDT_test).reshape(-1))}\n",
    "\n",
    "df_training = pd.concat([pd.DataFrame(train_data), pd.DataFrame(validation_data)])\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# df_training.to_csv('../analyze_prediction_results/cgcnn_prediction_results/%s_training.csv'%''.join(composition_of_interest), sep='\\t')\n",
    "# df_test.to_csv('../analyze_prediction_results/cgcnn_prediction_results//%s_test.csv'''.join(composition_of_interest), sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
